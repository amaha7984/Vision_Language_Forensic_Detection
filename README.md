# Vision_Language_Forensic_Detection
A multimodal framework for forensic detection of synthetic images using Vision-Language models and contrastive learning techniques.

## Overview

This repository combines the power of **SimCLR-based self-supervised learning** and **vision-language contrastive learning** to detect synthetic images. The framework aligns visual and textual representations using robust embeddings generated by SimCLR and CLIP, optimized with contrastive loss functions.

## Features
- **Self-Supervised Learning**: SimCLR is used for robust image feature extraction.
- **Vision-Language Alignment**: CLIP's transformer-based text encoder aligns text and image modalities for improved detection.
- **Forensic Detection**: The model effectively classifies synthetic images using multimodal data.

---

## Training 

### 1. Train the SimCLR Model
SimCLR serves as the backbone for self-supervised feature extraction. To train the SimCLR model, run:
```bash
python SIMCLR/main.py
The trained weight will be saved in the SIMCLR/weights

### 2. Train the Vision-Language Forensic Detection Model
```bash
sh main.sh
This command loads the SimCLR weights and trains the complete Vision-Language model.

### Acknowledgments
This work is developed based on [LASTED]([https://github.com/taesungp/contrastive-unpaired-translation](https://github.com/HighwayWu/LASTED) and [simclr]([https://github.com/Blealtan/efficient-kan](https://github.com/google-research/simclr)). We appreciate the great work provided by LASTED.

